---
title: "teaching my computer to nag me ü§ñ"
date: 2026-02-18
draft: false
---

been messing around with [OpenClaw](https://openclaw.ai) lately ‚Äî it's basically a self-hosted AI assistant that you can actually customize instead of just yelling at ChatGPT in a browser tab.

the premise is pretty simple: you run a little gateway service on your machine, connect it to whatever messaging apps you use (Discord, WhatsApp, Signal, etc), and then you've got an AI that can actually *do* stuff. like, proper tool use. git commits, API calls, file operations, the whole deal.

## why bother?

i've tried a bunch of AI assistant things over the past couple years and they all kinda suck in the same way: they're either too locked down to be useful, or they're so powerful that you're scared to give them access to anything real.

OpenClaw hits a nice middle ground. it's open source, runs locally, and has this neat approval system where the AI can *ask* to do scary things but you have to explicitly say yes. so like, if it wants to delete a file or push to GitHub, you get a little approval prompt first.

also i'm lazy and wanted something that could:
- remind me to go running every morning (with escalating passive-aggression)
- write code PRs for me
- manage my calendar/email without me having to context-switch into a browser

so i set it up! here's what that looked like.

## the docker compose setup

before jumping into what i configured, let me talk about the infrastructure that makes this all possible.

OpenClaw runs as a set of Docker services on my workstation. this is the important bit: everything runs locally in containers, which means:
- no data leaves my machine
- i can iterate on configuration without touching the host
- if something breaks, i just `docker-compose down` and try again
- i can run untrusted code (agent-generated commands) in a sandboxed environment

here's the stack:

### services

**Ollama** (local LLM inference server)
- runs on `localhost:11434`
- has direct access to my GPU (GTX 1080 Ti, 11GB VRAM)
- models live in a named volume so they persist across restarts
- one model at a time (8B parameter models eat ~8-9GB VRAM)
- **cost:** $0

**OpenClaw Gateway** (the orchestrator)
- the brains of the operation
- spawns agents, manages sessions, routes messages
- mounts the Docker socket for "Docker-outside-of-Docker" (DooD) ‚Äî lets it spin up sandbox containers on-demand
- runs with a read-only filesystem except for `/tmp` and the workspace
- bound to `127.0.0.1:18789`
- depends on Ollama being healthy before it starts

**OpenClaw Sandbox** (execution environment)
- build-only service; doesn't run as a persistent container
- gateway spawns sandbox instances on-demand for agent code
- based on `debian:bookworm-slim` with minimal tooling (bash, git, curl, python3, jq, ripgrep)
- runs as non-root user with dropped Linux capabilities
- resource-limited (memory, CPU)

**CLI** (local terminal access)
- lets me interact with the gateway from my shell
- not needed for the automated stuff, but useful for testing

### architecture diagram (in my head)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         My Workstation (Host)           ‚îÇ
‚îÇ  - Docker daemon                        ‚îÇ
‚îÇ  - ~/.openclaw/workspace/ (mounted)     ‚îÇ
‚îÇ  - GPU access                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  docker-compose       ‚îÇ
        ‚îÇ  (manages 4 services) ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚ñº           ‚ñº           ‚ñº          ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇOllama‚îÇ   ‚îÇ Gateway  ‚îÇ ‚îÇSandbox ‚îÇ ‚îÇ CLI ‚îÇ
    ‚îÇ :11434   ‚îÇ :18789   ‚îÇ ‚îÇ (on-   ‚îÇ ‚îÇ     ‚îÇ
    ‚îÇGPU ‚úì     ‚îÇDocker ‚úì  ‚îÇ ‚îÇ demand)‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ depends on
                    ‚ñº Ollama health
```

the key insight here is that the gateway spawns new sandbox containers whenever an agent needs to execute code. each sandbox is isolated, has limited resources, and gets destroyed after the task finishes. this is how untrusted code (generated by the AI) runs safely on my machine.

## what i configured

### 1. heartbeat reminders üèÉ‚Äç‚ôÇÔ∏è

this was the first thing i wanted: automated daily reminders to go run. not just a calendar notification that i'll swipe away ‚Äî something that will *keep bugging me* until i actually do it.

OpenClaw has this "heartbeat" system where the AI wakes up every 30 minutes and checks a `HEARTBEAT.md` file for tasks. i wrote a little state machine that:

- fires at 5:15am on run days (Tue/Wed/Thu/Sun per my marathon training plan)
- sends hourly reminders with escalating snark until i say "done"
- asks for an outdoor photo as proof (can verify it looks outside, but doesn't do facial recognition bc that's creepy)
- on rest days, sends one encouragement message then leaves me alone

the state lives in a JSON file in the workspace, so it persists across reboots. it's basically localStorage but for an AI agent lol.

> **fun detail:** the reminders are supposed to get "increasingly flagrant" with each hour. looking forward to seeing what my AI comes up with when i inevitably sleep through the first three üòÖ

checkbox time! what's working:
- [x] daily reminders fire at 5:15am
- [x] state persistence across heartbeat polls
- [x] photo verification (outdoor check only)
- [ ] actually going running consistently (work in progress)

### 2. model switching üí∞

OpenClaw supports a bunch of different LLM providers ‚Äî OpenAI, Anthropic, local Ollama models, etc. i started on `gpt-5.1-codex` (expensive, overkill) and switched to `gpt-4-turbo` after realizing i was burning $$ on simple tasks.

gpt-4-turbo is like 5-10x cheaper and handles tool calls just fine. the really cool part is you can configure different models for different agents, so in theory i could have:
- a cheap local model for simple stuff (file searches, log parsing)
- gpt-4-turbo for normal tasks
- something beefier (Claude Opus, GPT-5) for complex reasoning when i really need it

the GPU constraint is real though. my GTX 1080 Ti has 11GB VRAM, which means:
- Qwen 3:8b or Llama 3.1:8b fit fine (~8-9GB)
- can't run two models in parallel (would need 16GB+)
- gotta load/unload models on demand

so the cost optimization strategy looks like:
1. **check if task can use local Ollama** (file analysis, log parsing, simple formatting) ‚Üí $0, instant
2. **if local model is too slow or insufficient**, fall back to gpt-4-turbo ‚Üí ~$0.001-0.01 per call
3. **only use expensive models** (Claude Opus, GPT-5) for genuinely hard problems

in practice, i've been hitting gpt-4-turbo for like 80% of tasks and Ollama for 20%. total API spend: ~$2 over a full day of heavy use. would've been $10-15 with GPT-5.

> **GPU loading strategy:** since i can only run one Ollama model at a time, i keep Qwen loaded for fast file analysis, but swap to Llama if i need different strengths. loading a new model takes ~30 seconds. not ideal, but way cheaper than running everything on hosted APIs.

### 3. github PR automation üîß

wanted to test the "can it actually *do* things" part, so i had it:
- find my personal site repo (`k5m.sh`)
- analyze the current dark-mode-only setup
- implement a light/dark mode toggle with localStorage persistence
- write the CSS + JS
- create a feature branch
- commit the changes
- push to GitHub
- open a PR with a proper description

took maybe 10 minutes of back-and-forth and it got it right first try. [PR is here](https://github.com/khayyamsaleem/k5m.sh/pull/1) if you're curious!

the code it wrote is actually good? like, proper CSS variables for theming, a toggle button that remembers your preference, smooth transitions. i've shipped worse code myself.

> **security note:** i gave it a GitHub PAT (personal access token) to push, which worked but also made me realize i should probably rotate that token now lol. more on security below.

### 4. elevated exec (the scary part) üîì

this is where it gets spicy. by default, OpenClaw runs in a sandboxed Docker container with very limited access. but you can enable "elevated exec" which lets the AI run commands on the host machine.

i set mine to `elevated:ask` mode, which means:
- AI can request elevated commands
- i get an approval prompt with the full command visible
- i click yes/no
- command runs (or doesn't)

used this for:
- `openclaw config set` to change model settings
- `openclaw gateway restart` to reload config
- git operations (clone, push, etc)

it's actually pretty well-designed? the approval flow makes it feel safe enough to use without being paranoid, but scary enough that you don't blindly click through.

### 3. sandbox security üîí

here's where the Docker setup really matters. when the AI generates code, i don't want it to:
- delete important files
- steal credentials
- consume all my disk space
- fork bombs my system
- escape the container and mess with the host

so the sandbox has:
- **dropped Linux capabilities** ‚Äî no `CAP_SYS_ADMIN`, `CAP_NET_RAW`, etc. can't do privileged operations
- **memory limits** ‚Äî sandbox containers get max 1GB RAM, can't OOM the host
- **CPU limits** ‚Äî limited to 1 core, can't peg the CPU
- **user isolation** ‚Äî runs as non-root `sandbox` user (UID 1000), can't `chown` files
- **read-only root filesystem** ‚Äî `/` is immutable, only `/tmp` and `/workspace` are writable
- **network isolation** ‚Äî can talk to localhost but not arbitrary external hosts (unless i whitelist)

this all sounds paranoid but it's actually pretty reasonable. the AI can still do useful stuff (run commands, write files, git operations) but it can't nuke my system.

the workspace mount is the interesting bit. it's shared between the host and sandbox, which means:
- agent can read/write files in `~/.openclaw/workspace/`
- i can inspect what it did
- state persists across restarts
- but it's confined to that directory; it can't escape to `~/` or `/home/`

> **elevated exec note:** there's also an approval-based system for "elevated" commands that run on the host with full permissions. but those require my explicit yes before they execute. i use this for things like `openclaw config set` or `git push` where i want to make sure the command looks right before it runs.

---

## security stuff i learned üîí

while setting this up i did a little security review (wrote a whole doc at `/workspace/openclaw-security-review.md` if you care). here's the TLDR:

### what's good ‚úÖ
- **sandboxing**: AI runs in Docker, can't touch host filesystem without permission
- **approval system**: elevated commands need explicit yes from me
- **API key management**: keys live in env vars, not hardcoded
- **workspace isolation**: agent's files are in `~/.openclaw/workspace`, separate from everything else

### what's... not great ‚ö†Ô∏è
- **GitHub token exposure**: i embedded the PAT in the git remote URL (`https://TOKEN@github.com/...`), which means it's visible in `git remote -v` and process lists. dumb move. should use SSH keys instead.
- **secrets scattered everywhere**: API keys in env vars, tokens in config, credentials in random files. need to centralize this in `~/.openclaw/credentials/` or use OS keychain.
- **approval fatigue risk**: if i get too many approval prompts i'll start blindly clicking yes. need to audit the elevated command history and maybe whitelist safe commands.

nothing catastrophic, but def some cleanup needed. gonna rotate that GitHub token today.

### immediate todos:
- [ ] remove token from git remote config
- [ ] rotate GitHub PAT
- [ ] move all secrets to `~/.openclaw/credentials/`
- [ ] audit elevated exec logs
- [ ] switch to SSH keys for GitHub

## what's next

couple things i want to try:

1. **multi-agent delegation**: right now it's just one AI doing everything. but OpenClaw lets you spawn sub-agents with different models/capabilities. so i could have:
   - cheap local model (Ollama) for file scanning, log parsing, simple tasks
   - gpt-4-turbo (current) for normal work
   - Claude Haiku for creative writing tasks (like this blog post!)
   - something expensive for complex reasoning

   this would save $$ and also be way faster since sub-agents can run in parallel.

2. **email + calendar integration**: i've got `gog` (Google CLI) working, so i can query Gmail and Calendar. want to set up some automation like:
   - "find receipts for paid events and add them to my calendar"
   - "summarize my inbox every morning"
   - "remind me to follow up on emails i haven't replied to in 3 days"

3. **more heartbeat tasks**: the run reminder works great, so what else can i automate?
   - daily standup summaries
   - weekly goal check-ins
   - "go to bed" reminders when i'm still coding at 2am
   - location-based reminders (if OpenClaw can access my phone's location)

4. **skill system**: OpenClaw has this "skills" concept where you package up a task (markdown instructions + scripts + assets) and share it. want to contribute some skills back to the community once i've got more working.

## thoughts so far

honestly? this is the first AI assistant thing i've used that doesn't feel like a toy. it's genuinely useful, and the fact that it's self-hosted + open source means i'm not worried about:
- rate limits
- my data getting scraped for training
- the service shutting down or changing pricing
- "sorry, i can't do that" when i ask it to do something slightly weird

the approval system is clutch. i can give it real access (GitHub, file system, config) without feeling like i'm handing the keys to a toddler with a flamethrower.

setup was a little fiddly (had to manually configure models, tokens, etc), but once it's running it Just Works‚Ñ¢. and now that i've got the security stuff sorted, i'm way more comfortable letting it automate more of my life.

if you're the kind of person who:
- likes tinkering with self-hosted tools
- wants an AI that can actually *do things* instead of just chatting
- doesn't mind a little command-line config
- is comfortable with the "give AI access to your stuff" tradeoff

...then you should check out OpenClaw. it's legitimately cool.

---

**links:**
- [OpenClaw](https://openclaw.ai/)
- [OpenClaw GitHub](https://github.com/openclaw/openclaw)
- [my k5m.sh light mode PR](https://github.com/khayyamsaleem/k5m.sh/pull/1)

**cost so far:** ~$2 in API calls over a full day of heavy use (would've been $10-15 on GPT-5)

**will i still be using this in a month?** honestly yeah, probably. the run reminders alone are worth it lol üèÉ‚Äç‚ôÇÔ∏èüí®
