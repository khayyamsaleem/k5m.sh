+++
date = "18 Feb 2026"
title = "openclaw"
+++

## the journey: from frustration to optimization

i've been procrastinating on a lot of tedious computer stuff for months. you know the vibe -- things that take 5 minutes but require 15 minutes of context-loading to even start? like updating configs, merging PRs, rotating API keys, remembering to do literally anything on a schedule.

so i finally set up [OpenClaw](https://github.com/openclaw/openclaw) as my personal AI assistant in early February 2026. the initial setup worked, but then reality hit: the agent kept failing basic tasks, couldn't answer simple questions about my GPU, and didn't even know how to use the CLI tools i'd already installed. i spent a week frustrated, asking the same question four times, watching the agent struggle with things it should have been able to do out of the box.

that frustration drove everything that came after.

## why OpenClaw and not [insert other tool here]

couple reasons:
- **actually runs code**: most AI assistants are glorified chat interfaces. OpenClaw runs in a sandboxed Docker container and can actually execute commands, edit files, make git commits, etc.
- **multi-agent delegation**: can spawn sub-agents for specific tasks (though i haven't gotten deep into this yet)
- **self-hosted**: runs on my own infrastructure, not tied to some startup that'll pivot to NFTs or whatever
- **extensible**: can hook it up to Discord, Slack, SMS, or just run it headless

the main appeal for me was: i wanted something that could *do stuff* without me having to write custom scripts for every single workflow.

## what i configured: the infrastructure and initial setup

### 0. docker compose topology ‚Äî the container orchestration strategy üèóÔ∏è

before i jump into the optimizations, quick breakdown of what's actually running. i use Docker Compose with four services on the host machine (Manjaro Linux with a GTX 1080 Ti):

1. **Ollama** (local LLM inference) ‚Äî runs on the GPU, serves models like Qwen and Llama via HTTP on port 11434
2. **OpenClaw Gateway** (the orchestrator) ‚Äî handles agent routing, session management, sandboxing. mounts the Docker socket so it can spawn sandbox containers on-demand. API on port 18789
3. **OpenClaw CLI** ‚Äî interactive shell for direct agent prompts
4. **OpenClaw Sandbox** ‚Äî the execution boundary where untrusted code runs, with capabilities dropped (`capDrop: ["ALL"]`) and resource limits enforced (100 max processes, 2GB RAM, 2 CPU cores)

the Docker Compose topology uses **DooD** (Docker-outside-of-Docker): the gateway mounts `/var/run/docker.sock` to spawn sibling sandbox containers, and the workspace is double-mounted (at both the host-home path and `/workspace`) so all runtimes share the same files.

GPU setup: GTX 1080 Ti with 11GB VRAM, NVIDIA driver 575.64.05, CUDA 12.9. the constraint is **one model at a time** ‚Äî 8B models eat 5-6GB each, so loading a second model forces the first to swap out.

network: everything's on localhost (127.0.0.1). nothing exposed to the internet.

sandbox security: Docker isn't a cryptographic boundary ‚Äî if someone has kernel exploits, game over. but for defense-in-depth against script-kiddies and accidental mishaps, we've got capabilities dropped, resource limits, non-root user, and read-only gateway filesystem. it's designed to prevent accidental `rm -rf /` or malware exfiltration. it's good enough for personal use.

### 0.1 the initial fallback chain: overpowered and expensive

when i first set up OpenClaw in early February, my agent fallback chain was bloated:

```
GPT-4 Turbo ‚Üí Claude Sonnet ‚Üí Gemini Flash ‚Üí Qwen (local)
```

this was expensive and overkill. i was paying for the most powerful commercial models even though the agent mostly does email checks, config edits, and reminder nudges ‚Äî tasks that don't need GPT-4-class reasoning. **the frustration**: i wanted to optimize cost without sacrificing reliability.

## the frustration point: february 18th, session 3

on the evening of February 18th, during my third agent session, everything came to a head. i asked the agent to summarize my emails ‚Äî a basic daily task ‚Äî and it failed. four times. the agent didn't know what `gog` was. the same with weather requests. and when i asked it to check the GPU status, it couldn't ‚Äî `nvidia-smi` wasn't available in the sandbox.

the agent also tried to fetch a tool that didn't exist, asked for credentials that were already configured, and when i asked it to patch the OpenClaw config itself, it said it couldn't because it had no permission to use the `gateway` tool.

**the core problems, laid bare:**
1. Models didn't understand that `gog` was a pre-installed CLI tool ‚Äî they thought it was a native API
2. The GPU was installed on the host machine but invisible inside the containers
3. The agent didn't have access to the tools it needed to self-service config changes
4. The system prompt was enormous (~32K chars), making every single model less capable
5. Local models (Qwen) wrote bash in code blocks instead of calling the `exec` tool
6. Groq-hosted models were rejecting the payloads the agent sent them

this wasn't one problem. this was a cascade of problems, each one forcing me to manually intervene or work around the agent. **i decided to fix all of it.**

---

## the optimization narrative: breaking the cascade

### 1. GPU passthrough: "why can't the agent see the GPU?" üéÆ

**the frustration:** i asked the agent to run `nvidia-smi` in the sandbox. it couldn't. the GPU was sitting right there on the host machine (GTX 1080 Ti, 11GB VRAM), but the containers had no access to it. i wanted to profile local Ollama models, check VRAM usage, and verify that GPU inference was actually happening.

**what we did:** added GPU passthrough through the full OpenClaw config pipeline ‚Äî types, validation, docker args.

```typescript
// types.docker.ts ‚Äî added gpus field
export type SandboxDockerConfig = {
  gpus?: string;
};

// docker.ts ‚Äî passed to docker create
if (params.cfg.gpus) {
  args.push("--gpus", params.cfg.gpus);
}
```

config in `openclaw.json`:

```json
{ "agents": { "defaults": { "sandbox": { "docker": { "gpus": "all" } } } } }
```

**what we accomplished:** verified end-to-end GPU access in both runtimes.

```bash
# Gateway
docker exec open-claw-openclaw-gateway-1 nvidia-smi
# ‚Üí NVIDIA GeForce GTX 1080 Ti, 11GB VRAM, CUDA 12.9 ‚úì

# Sandbox (via agent)
# agent successfully ran nvidia-smi via exec ‚úì
```

### 2. groq API compatibility: "why are groq models returning 400 errors?" üîß

**the frustration:** when i added Groq-hosted models (GPT-OSS 120B, Maverick, etc.) to the config, they returned 400 errors. Groq's API is OpenAI-compatible but rejects certain parameters that OpenClaw's agent runner sends by default ‚Äî `include`, `parallel_tool_calls`, `reasoning_effort` for non-reasoning models. without this fix, no Groq model could work at all.

**what we did:** added `createGroqParamFilterWrapper()` that intercepts outgoing payloads and strips/normalizes Groq-incompatible params:

```typescript
function createGroqParamFilterWrapper(baseStreamFn, supportsReasoning) {
  return (model, context, options) => {
    onPayload: (payload) => {
      delete p.include;                    // Groq rejects this
      p.parallel_tool_calls = false;       // Groq fails with parallel
      if (!supportsReasoning) {
        delete p.reasoning_effort;         // Strip for non-reasoning models
        delete p.reasoning_format;
      }
    }
  };
}
```

**what we accomplished:** all Groq models now work reliably, unlocking 7 different models at a fraction of the cost.

### 3. the skill documentation breakthrough: "why don't models use gog?" üìö

**the frustration:** this was the one that drove the benchmarking journey. models had no idea that `gog` was a pre-installed CLI tool. they treated it as either a native API requiring special setup, or they just asked if it was installed. the workspace files never explicitly said "these are CLI tools, just run them via exec."

**the insight:** benchmarking 6 models on the same prompt ‚Äî "summarize my last 5 emails" ‚Äî revealed the root cause. pass rate was only 25% (1 out of 4 models). only GPT-OSS 120B figured it out. Haiku asked "is gog installed?", Maverick tried invoking it as a native API, Kimi suggested running `openclaw login`.

**what we did:** added 3 lines to `TOOLS.md`:

```markdown
## Skills = CLI Tools (Important)

Skills describe **CLI tools that are pre-installed and authenticated**.
Run them via the `exec` tool ‚Äî they are NOT native API tools.
Do not ask if they're installed; just run the command.

- **gog**: Pre-authenticated Google Workspace CLI. Just run
  `gog gmail search ...` via exec.
```

**what we accomplished:** re-ran the same 6 models with the updated TOOLS.md. pass rate: **83% (5 out of 6)**. a single documentation change moved the needle from 25% to 83%.

**benchmarking methodology:** to validate this wasn't a fluke, we created a 5-tier benchmark:
1. Direct Ollama API ‚Äî raw inference speed
2. Comprehensive evaluation ‚Äî 10 tests across coding, reasoning, summarization, tool-calling
3. Advanced agentic ‚Äî native tool-calling, large-context summarization
4. End-to-end gateway integration ‚Äî full agent turn
5. Real-world task ‚Äî "check email from the last 2 days, install gog if needed" (207 seconds on local Qwen)

each benchmark required isolated VRAM (using `ollama stop <model>` between runs) to ensure fair comparison. the real-world validation beat any synthetic benchmark for proving "this actually works."

**the results table (round 2 after TOOLS.md fix):**

| Model | Pass? | Notes |
|-------|-------|-------|
| GPT-OSS 120B | PASS | Fast, clean, 1-2 turns |
| Haiku 4.5 | PASS | Clean, correct gog invocation |
| Maverick | PASS | Clean (was 5-turn struggle before) |
| Kimi K2 | PASS | Clean, included timestamps |
| Qwen 3 8B (local) | PASS | Correct but slow (~2-3 min) |
| Qwen 3 32B (Groq) | FAIL | Tool-calling issue |

### 4. local model tool-call compliance: "why does qwen just write bash instead of calling exec?" üíª

**the frustration:** when i asked a local Qwen subagent to check the weather, it responded with a markdown code block instead of actually calling the `exec` tool. the user asked: *"Can you guarantee the ollama model actually did that work?"* ‚Äî it hadn't. Small local models consistently wrote bash commands as text instead of emitting tool calls.

**what we did:** two changes to force tool-call compliance:

```typescript
// ollama-stream.ts ‚Äî injected before every generation turn
const TOOL_CALL_REMINDER =
  "[Reminder: To execute commands, you MUST use tool calls. " +
  "Do NOT write ```bash code blocks ‚Äî they do nothing. Call the exec tool instead.]";

// system-prompt.ts ‚Äî strengthened globally
"## Tool Call Style (MANDATORY)",
"ALWAYS use tool calls to perform actions.",
"NEVER write shell commands in markdown code blocks ‚Äî that does nothing.",
```

**what we accomplished:** `ollama/qwen3:8b` now passes the email summarization benchmark. free local inference that actually works.

### 5. the agent workspace resolution bug: "why does each agent use the wrong directory?" üóÇÔ∏è

**the frustration:** non-default agents (`coder`, `budget`, `qwen`) have their own workspace directories, but the agent runner was using `process.cwd()` for all of them. this meant context injection and read-path auditing used the wrong directory.

**what we did:**

```typescript
// agent-runner.ts
// BEFORE: const workspaceDir = process.cwd();
// AFTER:
const workspaceDir =
  resolveAgentWorkspaceDir(cfg, resolveAgentIdFromSessionKey(sessionKey)) ??
  process.cwd();
```

**what we accomplished:** per-agent workspace directories now resolve correctly, so workspace files (AGENTS.md, TOOLS.md, SOUL.md) are injected from the right location for each agent.

### 6. context optimization: "32K chars of system prompt every single turn?" üíæ

**the frustration:** even after the TOOLS.md fix, we realized that system prompt was massive (~32K chars / ~13.5K tokens). worse: **Groq has no prompt caching** ‚Äî full cost every single turn. every character is sent and billed. the user asked us to "investigate options to minimize the amount of context agents need."

**what we did:** trimmed `AGENTS.md` from ~3K chars to ~800 chars by replacing prose explanations with a compact table:

```markdown
## Model Routing

| Agent | Model | Cost ($/MTok in/out) | Use For |
|-------|-------|---------------------|---------|
| `main` | GPT-OSS 120B (Groq) | $0.15 / $0.60 | Daily driver |
| `coder` | Kimi K2 (Groq) | $1.00 / $3.00 | Complex coding |
| `budget` | GPT-OSS 20B (Groq) | $0.075 / $0.30 | Cheap delegation |
| `qwen` | qwen3:8b (local) | $0 | Free tool-capable local model |

**Fallback chain:** GPT-OSS 120B ‚Üí Maverick ‚Üí Haiku 4.5 ‚Üí GPT-4o-mini
```

**enabled context pruning:**

```json
{
  "agents": {
    "defaults": {
      "contextPruning": {
        "mode": "cache-ttl",
        "ttl": "5m",
        "keepLastAssistants": 3,
        "softTrim": {
          "maxChars": 4000,
          "headChars": 1500,
          "tailChars": 1500
        }
      }
    }
  }
}
```

**system prompt breakdown (before optimization):**
- Workspace files: 17,311 chars (54%)
  - AGENTS.md: 7,323 ‚Üí 4,800 chars (trimmed)
  - TOOLS.md: 2,482 chars
  - SOUL.md: 2,316 chars
  - Other: ~2,900 chars
- Skills: 3,283 chars (10%)
- Framework chrome: 14,201 chars (44%)

**what we accomplished:** reduced per-turn cost by removing redundant prose and enabling automatic pruning of old tool results. more importantly, models now had a smaller prompt surface, making them more reliable.

### 7. gateway tool & exec host fix: "why can't the agent patch the config?" üîê

**the frustration:** on Feb 19, the user asked the agent to check channel status and patch the config. the agent tried running `openclaw channels status --probe` ‚Äî but got permission denied because the binary doesn't exist in the sandbox. the agent tried switching to the gateway host, but was blocked by policy. the agent tried using the `gateway` tool, but it was on the sandbox deny list. the agent was trapped and had to give up.

**what we did:** two config changes:

```json
{
  "tools": {
    "exec": {
      "host": "gateway"
    },
    "sandbox": {
      "tools": {
        "deny": ["canvas", "nodes", "discord"]
      }
    }
  }
}
```

1. **Set `tools.exec.host` to `"gateway"`** ‚Äî exec commands now run on the gateway (which has the OpenClaw CLI and all skill binaries) instead of the sandbox
2. **Removed `"gateway"` from the sandbox deny list** ‚Äî the agent can now use the `gateway` tool to read and patch `openclaw.json`, restart the gateway, etc.

**updated TOOLS.md with the correct pattern:**

```markdown
## OpenClaw Config Editing

Use the **`gateway` tool** to read and modify `openclaw.json`.

- **Read config:** `gateway` tool with `action: "config.get"`
- **Patch config:** `gateway` tool with `action: "config.patch"`,
  `raw: '{"tools":{"exec":{"host":"gateway"}}}'`
- **Restart gateway:** `gateway` tool with `action: "restart"`,
  `reason: "applied config change"`
```

**what we accomplished:** the agent can now self-service config changes and run CLI commands without manual intervention.

---

## the final state: model lineup and cost projection

after all the optimizations, the final agent lineup looked like this:

| Agent | Model | Cost ($/MTok in/out) | Role |
|-------|-------|---------------------|------|
| `main` | GPT-OSS 120B (Groq) | $0.15 / $0.60 | Daily driver ‚Äî 83% pass rate on benchmark |
| `coder` | Kimi K2 (Groq) | $1.00 / $3.00 | Complex coding tasks |
| `budget` | GPT-OSS 20B (Groq) | $0.075 / $0.30 | Cheap delegation, high-volume requests |
| `qwen` | qwen3:8b (local) | $0 | Free, fully local inference |

**fallback chain (if primary fails):**
GPT-OSS 120B ‚Üí Maverick (Groq) ‚Üí Haiku 4.5 (Anthropic) ‚Üí GPT-4o-mini (OpenAI)

**projected cost:** ~$27/month (well under budget).

all of this came from understanding what the agent actually does (via `/session-messages` custom command to read session logs

), analyzing patterns across 50+ recent messages, and identifying which tasks consumed the most agent time. email summarization was the winner ‚Äî it's the hardest benchmark (requires understanding `gog` is a CLI tool, invoking it via `exec`, and summarizing the output).

---

## daily use cases and security

### daily run reminders via heartbeat system üèÉ

i run 4x/week (Tue/Wed/Thu/Sun at 5:15am) and i'm extremely bad at remembering. so i set up a heartbeat system that:
- checks the current day/time
- sends me a Discord DM if it's a run day
- escalates if i don't acknowledge
- tracks state in a JSON file so it doesn't spam me

here's the vibe from `HEARTBEAT.md`:

```markdown
## Schedule
Run days: Tuesday, Wednesday, Thursday, Sunday
Reminder time: 5:15 AM ET

## What It Does
- 5:15 AM: "Morning! Run day today üèÉ"
- 6:00 AM: "Reminder: run day!" (if not acknowledged)
- 7:00 AM: "Last call for morning run!" (if still not acknowledged)
```

it's literally just localStorage vibes for my brain. the agent reads/writes to `run_reminder_state.json` and keeps track of whether i've been reminded today. extremely simple, extremely useful.

### GitHub PR automation üîß

i wanted to add a light/dark mode toggle to this blog (k5m.sh). normally this would involve:
1. cloning the repo
2. editing the Hugo templates
3. writing some JavaScript
4. testing locally
5. committing and pushing
6. opening a PR

instead i asked the agent to do it, and it handled the whole workflow in 2 minutes. i reviewed the PR, merged it, and boom -- light mode.

### elevated exec with approval mode üîê

by default, the agent runs in a sandboxed Docker container with limited permissions. but sometimes i need it to edit the OpenClaw config itself, or restart the gateway. when i say "arbitrary commands," i mean commands that run in the gateway context (not the sandbox) ‚Äî like restarting services, modifying config files, or reading system state. all of that requires explicit approval.

so i set up `elevated:ask` mode, which means:
- agent can request elevated (gateway) execution
- i get a real-time Discord prompt with the exact command
- i approve or deny
- command runs (or doesn't) depending on approval

the gateway has a timeout on the approval window (configurable, defaults to 5 minutes). if i don't respond, the agent moves on. this has been super useful for quick config tweaks without having to SSH into the box myself.

example flow:
```
Agent: "I need to update the primary model config. 
        Command: `openclaw config set agents.defaults.model.primary openai/gpt-oss-120b`
        Approve? [yes/no]"

Me: "yes" (in Discord, within 5 minutes)

Agent: "‚úÖ Config updated!"
```

## the full security model: defense-in-depth üîí

after the initial setup, i discovered that the agent needed to access GitHub, Google Workspace, Discord, and manipulate its own configuration. each of those touches secrets (API keys, PATs, OAuth tokens). i implemented a multi-layer defense strategy based on threat modeling.

### threat vectors we're protecting against

**accidental token commit to Git**
- `.gitignore` entries for `.env` and all `.env.*.local` files
- pre-commit hooks that scan for token patterns (`sk-*`, `ghp_*`, `xoxb-*`) before staging
- monthly secret scan script that audits git history + disk for leaked patterns

**malicious agent code execution**
- Docker sandbox with dropped capabilities (`capDrop: ["ALL"]`) ‚Äî agent can't call privileged syscalls
- resource limits: 100 max processes, 2GB RAM, 2 CPU cores
- unprivileged user (no root)
- `elevated:ask` approval gate for host-level operations
- network isolation via firewall rules

**token exfiltration via process logs or URLs**
- SSH keys for GitHub (not embedded PATs) ‚Äî not visible in URLs or process lists
- immutable `.env` file (`chattr +i` on Linux)
- env vars loaded with `set +x` ‚Äî no echo to logs
- encrypted disk (dm-crypt)
- token rotation schedule: 90-day cycle for API keys

**log file leaks**
- log rotation + archival ‚Äî deleted after 30 days
- cold storage backup ‚Äî encrypted `.env` backed up offline
- secure deletion (`shred -vfz`) for rotated tokens
- read-only gateway filesystem (except `/tmp` and mounts)

**supply chain attacks**
- `npm ci` (not `npm install`) ‚Äî locks to exact versions
- `npm audit` in pre-commit hooks
- version pinning ‚Äî no wildcards
- selective code review before installation

### what actually went into production

**SSH keys instead of tokens for GitHub**
```bash
ssh-keygen -t ed25519 -f ~/.ssh/github
git config --global url."git@github.com:".insteadOf "https://github.com/"
```

**immutable .env**
```bash
chmod 600 ~/.env
sudo chattr +i ~/.env  # Linux; chflags uchg on macOS
```

**network isolation via firewall**
```bash
sudo ufw default deny outgoing
sudo ufw allow out to api.openai.com port 443
sudo ufw allow out to api.github.com port 443
# (+ Ollama on localhost)
```

**token rotation schedule**
- OpenAI API key: 90 days
- GitHub SSH key: 90 days
- Discord bot token: 365 days (or immediately if compromise suspected)

**log scrubbing (monthly)**
```bash
grep -r 'sk-[A-Za-z0-9]\{20,\}' /logs/
grep -r 'ghp_[A-Za-z0-9]\{36,\}' /logs/
```

### threat level assessment

| Scenario | Severity | Defended? |
|----------|----------|-----------|
| Accidental token leak (GitHub commit) | high | ‚úÖ |
| Malicious LLM output (exfiltration) | high | ‚úÖ |
| Token visible in process/logs | high | ‚úÖ |
| Supply chain attack (npm) | medium-high | ‚úÖ |
| Disk forensic recovery | medium | ‚úÖ |
| Kernel exploit / sandbox escape | low | ‚ö†Ô∏è (defense-in-depth only) |

**Honest assessment:** we're defending against realistic scenarios ‚Äî accidents, lazy mistakes, script-kiddies, supply chain attacks. we're **not** defending against nation-state actors with kernel exploits or someone who already has root. if they have root, all security is theater. but for a personal machine where you control the code, this setup is solid.

## the complete timeline: from frustration to optimization

**Feb 15‚Äì17:** Initial OpenClaw setup. Works, but barely. Agent can't find tools, doesn't know how to use CLI utilities, can't access GPU info.

**Feb 18 (evening, Session 3):** The frustration peak. Four failed email summaries. GPU visibility issue. Agent can't self-service config changes. Decision made: fix everything.

**Feb 18‚Äì19 (Claude Code sessions):**
- Session 1: GPU passthrough (Dockerfile config + types + validation)
- Session 2: Groq API compatibility (param filtering for incompatible models)
- Session 3: Benchmarking discovery (why 25% pass rate on email summarization?)
- Session 4: TOOLS.md rewrite (the 3-line "Skills = CLI Tools" breakthrough)
- Session 5: Re-benchmark with new TOOLS.md (83% pass rate, vindication)
- Session 6: Context optimization (trim AGENTS.md, enable pruning, reduce per-turn cost)
- Session 7: Gateway tool + exec host fix (agent can now self-service config)
- Session 8: Local model tool-call compliance (Qwen reminder injection)
- Session 9: Workspace resolution bug (per-agent directory fixes)

**Feb 19 (implementation complete):**
- All changes merged to main OpenClaw repo
- Config finalized with 12 models across 4 providers
- 4 agents deployed with benchmark-informed fallback chain
- Security model deployed (SSH keys, immutable .env, log scrubbing, rotation schedule)
- Workspace docs updated (TOOLS.md, AGENTS.md, SOUL.md)

**Post-Feb 19 (ongoing):**
- Agent running reliably with 83% task success rate
- Cost at ~$27/month (within budget)
- Zero token leaks or security incidents
- Heartbeat reminders working (haven't missed a run day)
- GitHub automation, email summarization, config patching all working

---

## the key insights from this journey

**1. Documentation is more powerful than you think.** A 3-line addition to TOOLS.md moved the needle from 25% to 83% pass rate. Models needed to be _told_ that gog was a CLI tool, not given hints.

**2. Benchmarking on real workflows matters.** Email summarization is harder than "write a poem" ‚Äî it requires understanding tool-calling, execution context, and integration patterns. Small sample sizes (10 tests) are fine if the tests are realistic.

**3. Context size is a bottleneck for Groq.** No prompt caching means every character costs money every turn. Trimming AGENTS.md from 3K to 800 chars reduced per-turn cost without losing information (just changed format).

**4. Local models need explicit tool-call compliance.** Small models (5-8B parameters) don't understand implicit conventions. They need the system prompt to say "MUST use tool calls, NOT code blocks."

**5. GPU passthrough in Docker requires upfront config work.** But once it's in the pipeline (types ‚Üí validation ‚Üí docker args), it just works. Spend the time to do it right, not as a one-off hack.

**6. Self-service config changes require deliberate permission boundaries.** The agent needed access to the `gateway` tool to patch openclaw.json, and exec needed to run on the gateway host (where binaries live), not in the sandbox. These decisions cascade through the security model.

---

## what's next

- [ ] **multi-agent delegation**: spawn sub-agents for specific tasks (e.g. one for monitoring, one for GitHub ops, one for reminders)
- [ ] **more automation**: hook up to more services (calendar, email, SMS, home automation?)
- [ ] **further context optimization**: experiment with tool profiles (different toolsets per agent) to reduce schema size
- [ ] **extended benchmarking**: test against more realistic multi-turn tasks (e.g. "review this PR, run the tests, report findings")

the biggest next step is just... using it more. the more i lean on the agent for tedious stuff, the more workflows i discover that could be optimized.

## would i recommend it?

if you:
- are comfortable with Docker, SSH, and config files
- have a lot of repetitive computer tasks
- want an AI assistant that can actually *do stuff* (not just chat)
- are willing to spend a week or two optimizing the setup

then yes, OpenClaw is solid. it's not plug-and-play, but once you get it set up and optimize for your specific use case, it's surprisingly useful.

if you just want something to answer questions or help with writing, stick with ChatGPT or Claude. but if you want a robot friend that can commit code, send you reminders, rotate your API keys, and learn from each optimization cycle? OpenClaw is the move.

the key is: **don't expect it to work perfectly out of the box.** Expect to spend a week or two debugging, benchmarking, and tweaking. but if you're the kind of person who enjoys that optimization process (and you are, if you got this far in the post), you'll get a lot of value out of it.

---

## links + resources

- **OpenClaw repo**: https://github.com/openclaw/openclaw
- **Optimization log**: `/home/khayyam/.openclaw/workspace/OPTIMIZATION_LOG.md` (details on every change)
- **Benchmark methodology**: Section 3 of the OPTIMIZATION_LOG.md
- **Workspace files**: `TOOLS.md`, `AGENTS.md`, `SOUL.md`, `HEARTBEAT.md`
- **Security review**: Full threat modeling + implementation details above

if you set this up and run into issues, feel free to reach out! i'm `@khayyamsaleem` on most places.
